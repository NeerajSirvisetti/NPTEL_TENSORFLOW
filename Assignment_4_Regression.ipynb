{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 4 - Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeerajSirvisetti/NPTEL_TENSORFLOW/blob/master/Assignment_4_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLVhCarMwG70",
        "colab_type": "text"
      },
      "source": [
        "### **Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teXJ1XpSwdvR",
        "colab_type": "text"
      },
      "source": [
        "Install and import all the necessary libraries for the assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQNMrFD-ZBwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "d5a42be7-57e0-42b3-9471-ebd68a662617"
      },
      "source": [
        "!pip install tensorflow==2.0.0-rc0\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_boston\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "tf.random.set_seed(1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.0.0-rc0 in /usr/local/lib/python3.6/dist-packages (2.0.0rc0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.1.7)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (3.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.33.6)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (3.7.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.0.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.16.5)\n",
            "Requirement already satisfied: tb-nightly<1.15.0a20190807,>=1.15.0a20190806 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.15.0a20190806)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.1.0)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019080602,>=1.14.0.dev2019080601 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.14.0.dev2019080601)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-rc0) (41.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0-rc0) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0-rc0) (0.15.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0-rc0) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGGgAUOKwsWA",
        "colab_type": "text"
      },
      "source": [
        "### **Importing the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOe2azQOdmND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boston_dataset = load_boston()\n",
        "\n",
        "data_X = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
        "data_Y = pd.DataFrame(boston_dataset.target, columns=[\"target\"])\n",
        "data = pd.concat([data_X, data_Y], axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gD5esSxfxjs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "21ce3781-f11d-4aaf-d704-1c1c5726e018"
      },
      "source": [
        "train, test = train_test_split(data, test_size=0.2, random_state=1)\n",
        "train, val = train_test_split(train, test_size=0.2, random_state=1)\n",
        "print(len(train), \"train examples\")\n",
        "print(len(val), \"validation examples\")\n",
        "print(len(test), \"test examples\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "323 train examples\n",
            "81 validation examples\n",
            "102 test examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZTeC55HxDeT",
        "colab_type": "text"
      },
      "source": [
        "Converting the Pandas DataFrames into Tensorflow Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF4GRPPLdTIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('target')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdZy7p3AaTRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "train_ds = df_to_dataset(train, True, batch_size)\n",
        "val_ds = df_to_dataset(val, False, batch_size)\n",
        "test_ds = df_to_dataset(test, False, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOchKsY9MfEe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "51a27e4b-b520-4185-f2d5-6a8eab498f6d"
      },
      "source": [
        "data_X.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      CRIM    ZN  INDUS  CHAS    NOX  ...  RAD    TAX  PTRATIO       B  LSTAT\n",
              "0  0.00632  18.0   2.31   0.0  0.538  ...  1.0  296.0     15.3  396.90   4.98\n",
              "1  0.02731   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  396.90   9.14\n",
              "2  0.02729   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  392.83   4.03\n",
              "3  0.03237   0.0   2.18   0.0  0.458  ...  3.0  222.0     18.7  394.63   2.94\n",
              "4  0.06905   0.0   2.18   0.0  0.458  ...  3.0  222.0     18.7  396.90   5.33\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63KuTr4sxMl6",
        "colab_type": "text"
      },
      "source": [
        "### Defining Feature Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "380jHjPokFUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define feature_columns as a list of features using functions from tf.feature_column\n",
        "\n",
        "features = ['CRIM','ZN',\t'INDUS',\t'CHAS',\t'NOX',\t'RM',\t'AGE',\t'DIS',\t'RAD',\t'TAX',\t'PTRATIO',\t'B',\t'LSTAT']\n",
        "feature_columns = []\n",
        "for feature in features:\n",
        "  feature_columns.append(tf.feature_column.numeric_column(key=feature)) \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykVCMrdMxVB5",
        "colab_type": "text"
      },
      "source": [
        "### Building the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAc9LpVzqql9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6B9FgRyyGXe",
        "colab_type": "text"
      },
      "source": [
        "Model should contain following layers:\n",
        "\n",
        "```\n",
        "feature_layer\n",
        "\n",
        "Dense(1, activation=None)\n",
        "```\n",
        "\n",
        "Use 'Adam' optimizer\n",
        "\n",
        "Use 'mse' as your loss and metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZInuZ8D0xsu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build and compile your model in this cell.\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "def build_model():\n",
        "  model = keras.Sequential()\n",
        "  model.add(feature_layer),\n",
        "  model.add(layers.Dense(1, activation=None))\n",
        "  \n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "  model.compile(loss='mse',\n",
        "                optimizer=optimizer,\n",
        "                metrics=['mse'])\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNup-zg8YXWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igdzl3wasRo6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "88b12880-12c9-4309-bb62-86da11931296"
      },
      "source": [
        "model.fit(train_ds, validation_data=val_ds, epochs=200)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer sequential_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Epoch 1/200\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 173393.9957 - mse: 167335.8125 - val_loss: 0.0000e+00 - val_mse: 0.0000e+00\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 159181.4686 - mse: 159113.6406 - val_loss: 140690.2083 - val_mse: 141507.6250\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 151501.2089 - mse: 151186.7500 - val_loss: 133654.5625 - val_mse: 134430.5781\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 143921.1087 - mse: 143648.5625 - val_loss: 126910.0339 - val_mse: 127646.2812\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 136178.2151 - mse: 136360.0938 - val_loss: 120410.8568 - val_mse: 121109.6641\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 129061.5224 - mse: 129344.6719 - val_loss: 114054.6042 - val_mse: 114716.1094\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 122332.2147 - mse: 122427.5156 - val_loss: 107959.6328 - val_mse: 108584.5781\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 115778.8613 - mse: 115881.4844 - val_loss: 102103.6172 - val_mse: 102693.8359\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 109838.3215 - mse: 109643.2656 - val_loss: 96476.8203 - val_mse: 97033.5625\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 103604.5081 - mse: 103610.0938 - val_loss: 91206.3464 - val_mse: 91732.6406\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 98104.2120 - mse: 97889.2266 - val_loss: 86141.4089 - val_mse: 86638.7422\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 92325.5232 - mse: 92489.4297 - val_loss: 81299.0703 - val_mse: 81768.4922\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 87291.0657 - mse: 87236.6719 - val_loss: 76613.3359 - val_mse: 77055.8750\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 82401.8153 - mse: 82199.5859 - val_loss: 72189.7266 - val_mse: 72606.9375\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 77343.3471 - mse: 77425.3516 - val_loss: 68021.5078 - val_mse: 68415.0156\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 73081.3968 - mse: 72956.1016 - val_loss: 63930.5234 - val_mse: 64298.4688\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 68741.1031 - mse: 68608.7500 - val_loss: 60081.4714 - val_mse: 60425.7344\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 64364.1504 - mse: 64446.2148 - val_loss: 56473.1250 - val_mse: 56796.2344\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 60488.2333 - mse: 60542.5625 - val_loss: 52976.3815 - val_mse: 53279.0547\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 56860.0221 - mse: 56774.1133 - val_loss: 49665.1445 - val_mse: 49949.4102\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 53299.5058 - mse: 53235.9805 - val_loss: 46538.9180 - val_mse: 46805.4062\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 49987.8271 - mse: 49889.9688 - val_loss: 43568.8997 - val_mse: 43818.2891\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 46821.0095 - mse: 46726.3984 - val_loss: 40749.1458 - val_mse: 40982.2266\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 43687.4984 - mse: 43711.5156 - val_loss: 38078.8854 - val_mse: 38296.0117\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 40764.1903 - mse: 40825.3672 - val_loss: 35528.4036 - val_mse: 35730.4883\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 38053.1885 - mse: 38055.2539 - val_loss: 33117.7311 - val_mse: 33305.9180\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 35578.8250 - mse: 35456.8438 - val_loss: 30859.2858 - val_mse: 31034.4473\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 33105.9210 - mse: 33061.0430 - val_loss: 28750.3906 - val_mse: 28913.4688\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 30870.3327 - mse: 30803.8457 - val_loss: 26751.9219 - val_mse: 26903.4473\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 28671.8403 - mse: 28671.7500 - val_loss: 24868.0254 - val_mse: 25008.6113\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 26698.7794 - mse: 26627.6758 - val_loss: 23088.2064 - val_mse: 23218.4844\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 24798.9303 - mse: 24747.8770 - val_loss: 21417.4010 - val_mse: 21538.0273\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 23016.2165 - mse: 22955.4492 - val_loss: 19868.1855 - val_mse: 19980.2168\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 21338.7028 - mse: 21285.0996 - val_loss: 18424.8464 - val_mse: 18528.9824\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 19792.8396 - mse: 19738.0039 - val_loss: 17056.7653 - val_mse: 17153.0742\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 18273.7601 - mse: 18293.9102 - val_loss: 15759.2139 - val_mse: 15848.0684\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 16944.4971 - mse: 16895.4336 - val_loss: 14529.4603 - val_mse: 14611.1758\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 15644.5328 - mse: 15582.9893 - val_loss: 13411.5465 - val_mse: 13486.8359\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 14362.5558 - mse: 14394.3975 - val_loss: 12370.5098 - val_mse: 12439.6201\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 13247.9362 - mse: 13261.5322 - val_loss: 11378.6322 - val_mse: 11441.9023\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 12181.1354 - mse: 12202.3701 - val_loss: 10449.5111 - val_mse: 10507.2871\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 11207.0285 - mse: 11209.0469 - val_loss: 9581.1517 - val_mse: 9633.6748\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 10284.7238 - mse: 10277.4795 - val_loss: 8779.8854 - val_mse: 8827.6162\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9416.3649 - mse: 9433.5098 - val_loss: 8046.0317 - val_mse: 8089.6143\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8654.0699 - mse: 8645.4795 - val_loss: 7360.9800 - val_mse: 7400.6714\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7910.5435 - mse: 7916.7324 - val_loss: 6729.8400 - val_mse: 6765.8555\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7260.1326 - mse: 7236.3530 - val_loss: 6151.2012 - val_mse: 6183.8853\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6630.5063 - mse: 6628.3174 - val_loss: 5624.1895 - val_mse: 5653.9053\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6071.2276 - mse: 6060.0884 - val_loss: 5139.6543 - val_mse: 5166.4648\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5559.1412 - mse: 5549.0879 - val_loss: 4687.6947 - val_mse: 4711.9487\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5047.8169 - mse: 5062.2417 - val_loss: 4275.4844 - val_mse: 4297.2954\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4597.9560 - mse: 4616.3818 - val_loss: 3891.9126 - val_mse: 3911.5466\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4197.2568 - mse: 4203.3472 - val_loss: 3532.0166 - val_mse: 3549.4714\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3828.3872 - mse: 3822.3933 - val_loss: 3207.5580 - val_mse: 3223.1301\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3499.1904 - mse: 3483.0378 - val_loss: 2913.4430 - val_mse: 2927.3250\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3161.5309 - mse: 3174.5767 - val_loss: 2652.9963 - val_mse: 2665.4316\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2894.6276 - mse: 2889.7083 - val_loss: 2414.7132 - val_mse: 2425.8394\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2637.9497 - mse: 2636.8459 - val_loss: 2198.2715 - val_mse: 2208.2451\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2407.3729 - mse: 2403.7249 - val_loss: 1998.7866 - val_mse: 2007.6503\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2177.5050 - mse: 2190.7649 - val_loss: 1815.0252 - val_mse: 1822.8159\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1974.0340 - mse: 1991.7820 - val_loss: 1647.6751 - val_mse: 1654.5375\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1823.2283 - mse: 1812.5483 - val_loss: 1495.1078 - val_mse: 1501.1426\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1642.9194 - mse: 1652.3102 - val_loss: 1363.6720 - val_mse: 1368.9845\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1506.4456 - mse: 1509.4535 - val_loss: 1238.5120 - val_mse: 1243.1372\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1371.7577 - mse: 1375.6584 - val_loss: 1126.0436 - val_mse: 1130.0328\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1255.9413 - mse: 1256.0878 - val_loss: 1026.4086 - val_mse: 1029.8805\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1152.2105 - mse: 1150.0269 - val_loss: 939.4087 - val_mse: 942.4182\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1057.1322 - mse: 1058.3523 - val_loss: 861.8429 - val_mse: 864.4342\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 970.8411 - mse: 974.2233 - val_loss: 790.7626 - val_mse: 792.9599\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 890.8696 - mse: 897.3018 - val_loss: 727.8067 - val_mse: 729.6782\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 818.9731 - mse: 829.5959 - val_loss: 669.8166 - val_mse: 671.3821\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 758.5377 - mse: 765.9637 - val_loss: 617.2976 - val_mse: 618.5831\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 710.7595 - mse: 709.6048 - val_loss: 571.0063 - val_mse: 572.0682\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 658.5911 - mse: 661.0327 - val_loss: 529.9710 - val_mse: 530.8250\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 617.4104 - mse: 615.9717 - val_loss: 493.6623 - val_mse: 494.3273\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 572.6497 - mse: 576.8749 - val_loss: 463.2448 - val_mse: 463.7531\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 544.2544 - mse: 543.9716 - val_loss: 435.8944 - val_mse: 436.2648\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 517.6848 - mse: 513.9968 - val_loss: 412.3602 - val_mse: 412.6104\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 489.2061 - mse: 488.8562 - val_loss: 391.6118 - val_mse: 391.7603\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 466.0315 - mse: 466.9918 - val_loss: 372.3987 - val_mse: 372.4594\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 448.4937 - mse: 445.9934 - val_loss: 356.6491 - val_mse: 356.6391\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 431.2606 - mse: 428.0624 - val_loss: 343.0819 - val_mse: 343.0103\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 408.3438 - mse: 413.0146 - val_loss: 330.6029 - val_mse: 330.4762\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 401.8467 - mse: 398.7177 - val_loss: 318.4648 - val_mse: 318.3000\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 387.5254 - mse: 385.5826 - val_loss: 308.5862 - val_mse: 308.3847\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 377.6726 - mse: 374.7719 - val_loss: 300.1041 - val_mse: 299.8740\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 361.1423 - mse: 365.4070 - val_loss: 292.6781 - val_mse: 292.4223\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 358.2687 - mse: 356.7868 - val_loss: 285.8578 - val_mse: 285.5880\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 346.1033 - mse: 348.8947 - val_loss: 280.0162 - val_mse: 279.7309\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 342.7562 - mse: 342.3041 - val_loss: 274.1323 - val_mse: 273.8346\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 336.8820 - mse: 335.6155 - val_loss: 270.2827 - val_mse: 269.9881\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 328.2484 - mse: 330.9980 - val_loss: 266.5920 - val_mse: 266.2971\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 327.9789 - mse: 326.5754 - val_loss: 262.7848 - val_mse: 262.4874\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 323.9022 - mse: 322.2907 - val_loss: 259.3725 - val_mse: 259.0720\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 319.1531 - mse: 318.3805 - val_loss: 256.6072 - val_mse: 256.3096\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 315.2839 - mse: 315.0586 - val_loss: 254.1485 - val_mse: 253.8569\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 312.3913 - mse: 311.9948 - val_loss: 251.7203 - val_mse: 251.4357\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 310.9228 - mse: 309.1422 - val_loss: 249.7571 - val_mse: 249.4857\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 308.9280 - mse: 306.6168 - val_loss: 248.0140 - val_mse: 247.7519\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 306.2404 - mse: 304.5499 - val_loss: 246.5655 - val_mse: 246.3162\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 303.6173 - mse: 302.9424 - val_loss: 245.3792 - val_mse: 245.1528\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 303.1958 - mse: 301.4894 - val_loss: 244.3436 - val_mse: 244.1409\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 301.1953 - mse: 300.0714 - val_loss: 243.2590 - val_mse: 243.0745\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 300.0824 - mse: 298.7859 - val_loss: 242.2696 - val_mse: 242.1032\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 297.7460 - mse: 297.5419 - val_loss: 241.4015 - val_mse: 241.2574\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 295.9091 - mse: 296.3396 - val_loss: 240.5838 - val_mse: 240.4594\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 295.7501 - mse: 295.2515 - val_loss: 239.7640 - val_mse: 239.6632\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 295.8808 - mse: 294.3387 - val_loss: 239.0960 - val_mse: 239.0227\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 294.7029 - mse: 293.4019 - val_loss: 238.3584 - val_mse: 238.3125\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 294.3452 - mse: 292.4920 - val_loss: 237.6568 - val_mse: 237.6368\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 292.9194 - mse: 291.5763 - val_loss: 236.9254 - val_mse: 236.9245\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 286.6692 - mse: 290.6700 - val_loss: 236.3474 - val_mse: 236.3722\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 291.5241 - mse: 290.0369 - val_loss: 235.7536 - val_mse: 235.8165\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 289.9586 - mse: 289.3296 - val_loss: 235.1746 - val_mse: 235.2609\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 289.4394 - mse: 288.6305 - val_loss: 234.6362 - val_mse: 234.7566\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 289.8858 - mse: 288.0201 - val_loss: 234.0307 - val_mse: 234.1737\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 287.5580 - mse: 287.2165 - val_loss: 233.4509 - val_mse: 233.6203\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 284.7080 - mse: 286.4783 - val_loss: 232.9003 - val_mse: 233.0944\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 287.4322 - mse: 285.7966 - val_loss: 232.3590 - val_mse: 232.5908\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 282.2514 - mse: 285.1890 - val_loss: 231.8589 - val_mse: 232.1201\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 285.9098 - mse: 284.5358 - val_loss: 231.3367 - val_mse: 231.6226\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 285.2577 - mse: 283.9144 - val_loss: 230.8767 - val_mse: 231.1924\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 281.1316 - mse: 283.3742 - val_loss: 230.3869 - val_mse: 230.7306\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 280.1043 - mse: 282.7501 - val_loss: 229.8573 - val_mse: 230.2259\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 281.2049 - mse: 282.0256 - val_loss: 229.3553 - val_mse: 229.7467\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 280.2556 - mse: 281.3653 - val_loss: 228.7931 - val_mse: 229.2110\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 282.1569 - mse: 280.8157 - val_loss: 228.1836 - val_mse: 228.6413\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 277.1562 - mse: 280.1396 - val_loss: 227.6307 - val_mse: 228.1259\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 279.6268 - mse: 279.5409 - val_loss: 227.0442 - val_mse: 227.5860\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 273.7901 - mse: 278.9875 - val_loss: 226.5010 - val_mse: 227.0719\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 279.5122 - mse: 278.3019 - val_loss: 225.9301 - val_mse: 226.5195\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 279.2296 - mse: 277.7102 - val_loss: 225.3975 - val_mse: 226.0128\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 279.1476 - mse: 277.0518 - val_loss: 224.8817 - val_mse: 225.5244\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 275.7277 - mse: 276.4437 - val_loss: 224.4032 - val_mse: 225.0672\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 276.4284 - mse: 275.9378 - val_loss: 223.9039 - val_mse: 224.5897\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 275.1930 - mse: 275.2577 - val_loss: 223.4147 - val_mse: 224.1272\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 274.9656 - mse: 274.6771 - val_loss: 222.8415 - val_mse: 223.5849\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 276.1388 - mse: 274.0505 - val_loss: 222.3550 - val_mse: 223.1263\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 275.4180 - mse: 273.5014 - val_loss: 221.8301 - val_mse: 222.6337\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 273.4636 - mse: 272.9592 - val_loss: 221.2767 - val_mse: 222.1153\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 272.2239 - mse: 272.4208 - val_loss: 220.7552 - val_mse: 221.6263\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 271.1476 - mse: 271.8382 - val_loss: 220.1516 - val_mse: 221.0594\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 272.7641 - mse: 271.2701 - val_loss: 219.6343 - val_mse: 220.5824\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 271.9971 - mse: 270.7060 - val_loss: 219.1464 - val_mse: 220.1247\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 270.7820 - mse: 270.1875 - val_loss: 218.6432 - val_mse: 219.6493\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 271.1810 - mse: 269.5458 - val_loss: 218.1452 - val_mse: 219.1807\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 268.6950 - mse: 268.9433 - val_loss: 217.6164 - val_mse: 218.6850\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 270.2543 - mse: 268.5255 - val_loss: 217.1861 - val_mse: 218.2758\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 265.4199 - mse: 267.9067 - val_loss: 216.6159 - val_mse: 217.7441\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 268.0173 - mse: 267.2928 - val_loss: 216.0162 - val_mse: 217.1861\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 268.5835 - mse: 266.6942 - val_loss: 215.5619 - val_mse: 216.7606\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 266.1048 - mse: 266.1999 - val_loss: 215.0395 - val_mse: 216.2745\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 265.2312 - mse: 265.5884 - val_loss: 214.3783 - val_mse: 215.6625\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 265.4158 - mse: 265.0273 - val_loss: 213.7221 - val_mse: 215.0540\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 266.2005 - mse: 264.3665 - val_loss: 213.1990 - val_mse: 214.5620\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 264.0554 - mse: 263.8477 - val_loss: 212.6908 - val_mse: 214.0856\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 263.9037 - mse: 263.3371 - val_loss: 212.1741 - val_mse: 213.5970\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 264.0002 - mse: 262.6892 - val_loss: 211.5245 - val_mse: 212.9906\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 263.2984 - mse: 262.1158 - val_loss: 210.9899 - val_mse: 212.4942\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 260.4893 - mse: 261.5802 - val_loss: 210.4641 - val_mse: 212.0047\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 261.2423 - mse: 261.0228 - val_loss: 209.9331 - val_mse: 211.4975\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 255.7176 - mse: 260.3169 - val_loss: 209.3186 - val_mse: 210.9286\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 261.0038 - mse: 259.7421 - val_loss: 208.5936 - val_mse: 210.2739\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 258.3300 - mse: 259.2165 - val_loss: 207.9893 - val_mse: 209.7208\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 255.6932 - mse: 258.5903 - val_loss: 207.4709 - val_mse: 209.2161\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 259.4663 - mse: 258.0079 - val_loss: 207.1045 - val_mse: 208.8618\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 252.1370 - mse: 257.3506 - val_loss: 206.6578 - val_mse: 208.4367\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 257.8867 - mse: 256.7612 - val_loss: 206.3729 - val_mse: 208.1615\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 257.3444 - mse: 256.3270 - val_loss: 205.7949 - val_mse: 207.6233\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 257.4135 - mse: 255.7588 - val_loss: 205.2791 - val_mse: 207.1444\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 256.6584 - mse: 255.1976 - val_loss: 204.7462 - val_mse: 206.6496\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 256.0622 - mse: 254.6426 - val_loss: 204.1902 - val_mse: 206.1389\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 255.5866 - mse: 254.0802 - val_loss: 203.6142 - val_mse: 205.5991\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 253.4446 - mse: 253.4874 - val_loss: 203.1004 - val_mse: 205.1166\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 253.8834 - mse: 252.9713 - val_loss: 202.5771 - val_mse: 204.6293\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 253.8325 - mse: 252.3767 - val_loss: 201.9794 - val_mse: 204.0704\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 248.3220 - mse: 251.8096 - val_loss: 201.4551 - val_mse: 203.5807\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 253.1648 - mse: 251.1570 - val_loss: 200.9262 - val_mse: 203.0831\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 250.2337 - mse: 250.6685 - val_loss: 200.4701 - val_mse: 202.6516\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 251.4439 - mse: 250.1193 - val_loss: 199.9693 - val_mse: 202.1761\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 249.6299 - mse: 249.6357 - val_loss: 199.4291 - val_mse: 201.6711\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 249.9900 - mse: 248.8846 - val_loss: 198.7330 - val_mse: 201.0285\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 246.8556 - mse: 248.3434 - val_loss: 198.1581 - val_mse: 200.4967\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 246.7724 - mse: 247.7813 - val_loss: 197.6928 - val_mse: 200.0486\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 248.6537 - mse: 247.2100 - val_loss: 197.1625 - val_mse: 199.5535\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 245.6808 - mse: 246.6611 - val_loss: 196.5992 - val_mse: 199.0272\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 243.5290 - mse: 246.1026 - val_loss: 195.9616 - val_mse: 198.4410\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 245.9667 - mse: 245.4761 - val_loss: 195.5698 - val_mse: 198.0793\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 245.6959 - mse: 244.9571 - val_loss: 195.1652 - val_mse: 197.7034\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 246.1088 - mse: 244.4509 - val_loss: 194.5846 - val_mse: 197.1614\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 244.4196 - mse: 243.9138 - val_loss: 194.0802 - val_mse: 196.6909\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 239.2055 - mse: 243.3935 - val_loss: 193.6943 - val_mse: 196.3258\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 243.3086 - mse: 242.9859 - val_loss: 193.4005 - val_mse: 196.0496\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 242.9482 - mse: 242.4481 - val_loss: 192.8233 - val_mse: 195.5143\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 241.7150 - mse: 241.9344 - val_loss: 192.1415 - val_mse: 194.8756\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 241.9557 - mse: 241.2353 - val_loss: 191.6076 - val_mse: 194.3732\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 241.6215 - mse: 240.6299 - val_loss: 190.9431 - val_mse: 193.7580\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 239.8701 - mse: 240.1311 - val_loss: 190.3281 - val_mse: 193.2018\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 235.6793 - mse: 239.5141 - val_loss: 189.7521 - val_mse: 192.6837\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 239.9225 - mse: 238.9635 - val_loss: 189.1226 - val_mse: 192.1175\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb35d285668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFu2k4J_spfi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "ac435b89-d914-4cf7-8b60-779008c30ed8"
      },
      "source": [
        "loss, mse = model.evaluate(test_ds)\n",
        "print(\"Mean Squared Error - Test Data\", mse)\n",
        "print(\"loss - Test Data\", loss)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 59.2110 - mse: 58.9268\n",
            "Mean Squared Error - Test Data 58.926773\n",
            "loss - Test Data 59.211042404174805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITOBmGfeafRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#for 8 question\n",
        "# Build and compile your model in this cell.\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "def build_model():\n",
        "  model = keras.Sequential()\n",
        "  model.add(feature_layer),\n",
        "  model.add(layers.Dense(4, activation='sigmoid'))\n",
        "  model.add(layers.Dense(1, activation=None))\n",
        "  \n",
        "  \n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "  model.compile(loss='mse',\n",
        "                optimizer=optimizer,\n",
        "                metrics=['mse'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRCLxJJGcQOP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8e2fac42-488e-4d08-ecf0-85c0198371dd"
      },
      "source": [
        "model = build_model()\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=200)\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer sequential_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Epoch 1/200\n",
            "11/11 [==============================] - 1s 64ms/step - loss: 575.8638 - mse: 582.7656 - val_loss: 0.0000e+00 - val_mse: 0.0000e+00\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 580.9459 - mse: 580.8578 - val_loss: 548.4835 - val_mse: 544.4011\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 579.3398 - mse: 578.4832 - val_loss: 545.3681 - val_mse: 541.2708\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 571.1744 - mse: 572.0990 - val_loss: 537.7295 - val_mse: 533.9244\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 560.3155 - mse: 565.2466 - val_loss: 529.7370 - val_mse: 526.0458\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 557.1814 - mse: 553.6680 - val_loss: 515.9276 - val_mse: 511.8320\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 547.9387 - mse: 545.6450 - val_loss: 511.4122 - val_mse: 507.8525\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 543.7618 - mse: 542.4041 - val_loss: 510.2701 - val_mse: 506.7144\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 540.2140 - mse: 540.1938 - val_loss: 508.0360 - val_mse: 504.6087\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 538.9700 - mse: 536.8840 - val_loss: 505.6581 - val_mse: 502.2907\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 535.3825 - mse: 533.0273 - val_loss: 501.1982 - val_mse: 497.8611\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 530.0940 - mse: 529.7914 - val_loss: 498.8105 - val_mse: 495.2743\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 530.3622 - mse: 526.9301 - val_loss: 493.9854 - val_mse: 490.3942\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 522.1068 - mse: 522.8160 - val_loss: 490.9175 - val_mse: 487.4908\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 515.9613 - mse: 517.5999 - val_loss: 484.4687 - val_mse: 481.2155\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 515.9079 - mse: 513.1750 - val_loss: 479.5705 - val_mse: 476.5973\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 509.8083 - mse: 509.9368 - val_loss: 477.1401 - val_mse: 474.3466\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 508.6059 - mse: 507.3105 - val_loss: 474.9857 - val_mse: 472.1162\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 504.7650 - mse: 505.4028 - val_loss: 473.3500 - val_mse: 470.4904\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 504.3396 - mse: 503.2646 - val_loss: 471.7356 - val_mse: 468.8888\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 503.7503 - mse: 501.0981 - val_loss: 469.8680 - val_mse: 466.9788\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 501.8918 - mse: 499.2556 - val_loss: 468.2743 - val_mse: 465.3863\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 497.5445 - mse: 497.5273 - val_loss: 466.7838 - val_mse: 463.9107\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 493.8552 - mse: 495.8921 - val_loss: 465.2948 - val_mse: 462.4374\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 494.7627 - mse: 494.3230 - val_loss: 463.7876 - val_mse: 460.9463\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 492.6595 - mse: 492.7462 - val_loss: 462.3207 - val_mse: 459.4953\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 491.6287 - mse: 491.1934 - val_loss: 460.8885 - val_mse: 458.0783\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 489.0106 - mse: 489.6812 - val_loss: 459.4481 - val_mse: 456.6534\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 489.9696 - mse: 488.1136 - val_loss: 458.0060 - val_mse: 455.2268\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 485.2325 - mse: 486.4997 - val_loss: 456.6037 - val_mse: 453.8397\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 485.6173 - mse: 484.9637 - val_loss: 455.1725 - val_mse: 452.4241\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 484.2814 - mse: 483.4549 - val_loss: 453.7764 - val_mse: 451.0433\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 482.0316 - mse: 481.9825 - val_loss: 452.3978 - val_mse: 449.6796\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 476.7866 - mse: 480.5326 - val_loss: 451.0081 - val_mse: 448.3052\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 480.0454 - mse: 479.0775 - val_loss: 449.6045 - val_mse: 446.9170\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 479.6726 - mse: 477.6194 - val_loss: 448.2362 - val_mse: 445.5637\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 477.4163 - mse: 476.1928 - val_loss: 446.8915 - val_mse: 444.2337\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 470.1143 - mse: 474.7839 - val_loss: 445.5423 - val_mse: 442.8993\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 473.5783 - mse: 473.3445 - val_loss: 444.1588 - val_mse: 441.5307\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 470.0358 - mse: 471.9031 - val_loss: 442.8066 - val_mse: 440.1932\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 472.2787 - mse: 470.4828 - val_loss: 441.4578 - val_mse: 438.8594\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 471.3041 - mse: 469.0932 - val_loss: 440.1361 - val_mse: 437.5523\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 465.7630 - mse: 467.7100 - val_loss: 438.8343 - val_mse: 436.2646\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 467.7590 - mse: 466.3314 - val_loss: 437.5050 - val_mse: 434.9500\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 466.9298 - mse: 464.9514 - val_loss: 436.1996 - val_mse: 433.6586\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 466.1078 - mse: 463.5851 - val_loss: 434.9096 - val_mse: 432.3820\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 460.3462 - mse: 462.2368 - val_loss: 433.6249 - val_mse: 431.1102\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 462.2068 - mse: 460.8744 - val_loss: 432.3148 - val_mse: 429.8128\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 461.0397 - mse: 459.5356 - val_loss: 431.0087 - val_mse: 428.5185\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 460.2709 - mse: 458.1928 - val_loss: 429.7229 - val_mse: 427.2438\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 454.4880 - mse: 456.8567 - val_loss: 428.4247 - val_mse: 425.9519\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 455.3944 - mse: 455.5216 - val_loss: 427.1211 - val_mse: 424.6588\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 455.0875 - mse: 454.0298 - val_loss: 423.2533 - val_mse: 420.9442\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 451.4613 - mse: 450.6189 - val_loss: 419.4669 - val_mse: 416.7503\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 447.0371 - mse: 443.9691 - val_loss: 411.5154 - val_mse: 409.4952\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 427.7536 - mse: 436.2404 - val_loss: 402.5286 - val_mse: 400.3821\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 431.7464 - mse: 430.9932 - val_loss: 399.5670 - val_mse: 397.6077\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 427.8339 - mse: 427.9174 - val_loss: 397.3032 - val_mse: 395.3738\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 428.5478 - mse: 425.4192 - val_loss: 395.1121 - val_mse: 393.2117\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 422.9910 - mse: 423.0279 - val_loss: 393.0449 - val_mse: 391.1720\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 418.0306 - mse: 420.9092 - val_loss: 390.9741 - val_mse: 389.1288\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 420.1960 - mse: 418.8000 - val_loss: 388.9142 - val_mse: 387.0964\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 418.3434 - mse: 416.7144 - val_loss: 386.9742 - val_mse: 385.1824\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 417.0113 - mse: 414.7676 - val_loss: 385.0724 - val_mse: 383.3062\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 414.4073 - mse: 412.8267 - val_loss: 383.2434 - val_mse: 381.5019\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 405.0179 - mse: 410.9546 - val_loss: 381.4100 - val_mse: 379.6933\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 408.8762 - mse: 409.0384 - val_loss: 379.5177 - val_mse: 377.8267\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 407.8577 - mse: 407.0807 - val_loss: 377.6911 - val_mse: 376.0250\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 404.7252 - mse: 404.8964 - val_loss: 375.9303 - val_mse: 374.2882\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 403.5783 - mse: 403.0830 - val_loss: 374.1814 - val_mse: 372.5633\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 402.7960 - mse: 401.3068 - val_loss: 372.4425 - val_mse: 370.8482\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 400.2766 - mse: 399.5220 - val_loss: 370.7634 - val_mse: 369.1924\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 399.7781 - mse: 397.8172 - val_loss: 369.0720 - val_mse: 367.5243\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 395.7974 - mse: 396.0725 - val_loss: 367.4462 - val_mse: 365.9210\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 395.7483 - mse: 394.3927 - val_loss: 365.7973 - val_mse: 364.2950\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 393.5719 - mse: 392.7084 - val_loss: 364.1756 - val_mse: 362.6959\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 388.8618 - mse: 391.0550 - val_loss: 362.5481 - val_mse: 361.0912\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 384.3766 - mse: 389.3510 - val_loss: 360.9321 - val_mse: 359.4977\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 389.5969 - mse: 387.6663 - val_loss: 359.2955 - val_mse: 357.8842\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 388.0645 - mse: 386.0329 - val_loss: 357.7185 - val_mse: 356.3293\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 386.4272 - mse: 384.4303 - val_loss: 356.1838 - val_mse: 354.8163\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 382.6118 - mse: 382.8617 - val_loss: 354.6640 - val_mse: 353.3180\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 377.7076 - mse: 381.2726 - val_loss: 353.1570 - val_mse: 351.8324\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 381.6926 - mse: 379.7004 - val_loss: 351.6139 - val_mse: 350.3113\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 378.4533 - mse: 378.1328 - val_loss: 350.1204 - val_mse: 348.8392\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 378.6539 - mse: 376.5849 - val_loss: 348.6268 - val_mse: 347.3670\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 376.4091 - mse: 375.0887 - val_loss: 347.1467 - val_mse: 345.9081\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 369.9604 - mse: 373.5520 - val_loss: 345.6966 - val_mse: 344.4788\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 372.3139 - mse: 372.0176 - val_loss: 344.2106 - val_mse: 343.0143\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 367.0813 - mse: 370.5026 - val_loss: 342.7298 - val_mse: 341.5550\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 370.8669 - mse: 368.9767 - val_loss: 341.2503 - val_mse: 340.0969\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 367.2318 - mse: 367.4810 - val_loss: 339.8137 - val_mse: 338.6811\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 365.4978 - mse: 365.9759 - val_loss: 338.3811 - val_mse: 337.2695\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 365.6175 - mse: 364.4976 - val_loss: 336.9349 - val_mse: 335.8444\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 363.6267 - mse: 363.0382 - val_loss: 335.5037 - val_mse: 334.4341\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 362.2672 - mse: 361.5591 - val_loss: 334.0926 - val_mse: 333.0439\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 358.4396 - mse: 360.1002 - val_loss: 332.6902 - val_mse: 331.6621\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 356.3468 - mse: 358.6544 - val_loss: 331.2696 - val_mse: 330.2626\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 358.6659 - mse: 357.1909 - val_loss: 329.8495 - val_mse: 328.8635\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 358.2461 - mse: 355.7431 - val_loss: 328.4969 - val_mse: 327.5310\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 353.8873 - mse: 354.3541 - val_loss: 327.1697 - val_mse: 326.2235\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 353.8742 - mse: 352.9778 - val_loss: 325.7913 - val_mse: 324.8658\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 352.8997 - mse: 351.5562 - val_loss: 324.4462 - val_mse: 323.5408\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 350.8942 - mse: 350.1704 - val_loss: 323.1246 - val_mse: 322.2391\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 349.5487 - mse: 348.7895 - val_loss: 321.7959 - val_mse: 320.9304\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 346.9966 - mse: 347.4233 - val_loss: 320.4538 - val_mse: 319.6086\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 347.1846 - mse: 346.0420 - val_loss: 319.1066 - val_mse: 318.2818\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 345.7475 - mse: 344.6620 - val_loss: 317.7835 - val_mse: 316.9788\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 344.6859 - mse: 343.3062 - val_loss: 316.4692 - val_mse: 315.6845\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 342.8332 - mse: 341.9398 - val_loss: 315.1889 - val_mse: 314.4237\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 342.6760 - mse: 340.6237 - val_loss: 313.8901 - val_mse: 313.1448\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 336.9397 - mse: 339.2935 - val_loss: 312.6155 - val_mse: 311.8898\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 339.9261 - mse: 337.9666 - val_loss: 311.3115 - val_mse: 310.6059\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 338.1146 - mse: 336.6291 - val_loss: 310.0651 - val_mse: 309.3787\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 336.7700 - mse: 335.3457 - val_loss: 308.8031 - val_mse: 308.1363\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 334.2114 - mse: 334.0549 - val_loss: 307.5365 - val_mse: 306.8893\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 328.5207 - mse: 332.7316 - val_loss: 306.2740 - val_mse: 305.6464\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 328.3786 - mse: 331.3809 - val_loss: 304.9834 - val_mse: 304.3760\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 331.3281 - mse: 330.0619 - val_loss: 303.6861 - val_mse: 303.0989\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 324.6263 - mse: 328.7414 - val_loss: 302.4277 - val_mse: 301.8602\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 328.0755 - mse: 327.4456 - val_loss: 301.1419 - val_mse: 300.5946\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 326.7733 - mse: 326.1314 - val_loss: 299.9032 - val_mse: 299.3755\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 321.7469 - mse: 324.8500 - val_loss: 298.6776 - val_mse: 298.1693\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 319.7587 - mse: 323.5583 - val_loss: 297.4408 - val_mse: 296.9521\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 318.4433 - mse: 322.2755 - val_loss: 296.1651 - val_mse: 295.6967\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 321.0333 - mse: 320.9561 - val_loss: 294.8900 - val_mse: 294.4419\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 317.1479 - mse: 319.6677 - val_loss: 293.6709 - val_mse: 293.2423\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 315.5768 - mse: 318.3945 - val_loss: 292.4334 - val_mse: 292.0246\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 319.5972 - mse: 317.1245 - val_loss: 291.2116 - val_mse: 290.8225\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 316.8121 - mse: 315.8802 - val_loss: 290.0632 - val_mse: 289.6927\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 314.2016 - mse: 314.6884 - val_loss: 288.8908 - val_mse: 288.5391\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 310.1020 - mse: 313.4474 - val_loss: 287.7066 - val_mse: 287.3741\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 313.7457 - mse: 312.2244 - val_loss: 286.4935 - val_mse: 286.1808\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 309.4819 - mse: 310.9926 - val_loss: 285.3281 - val_mse: 285.0344\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 311.2640 - mse: 309.7708 - val_loss: 284.1497 - val_mse: 283.8752\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 308.8617 - mse: 308.5796 - val_loss: 282.9944 - val_mse: 282.7389\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 308.7762 - mse: 307.3759 - val_loss: 281.8517 - val_mse: 281.6150\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 307.6289 - mse: 306.2115 - val_loss: 280.7180 - val_mse: 280.4999\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 304.5760 - mse: 305.0222 - val_loss: 279.6127 - val_mse: 279.4129\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 304.7268 - mse: 303.8824 - val_loss: 278.4561 - val_mse: 278.2755\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 304.5424 - mse: 302.6888 - val_loss: 277.3351 - val_mse: 277.1730\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 303.3373 - mse: 301.5391 - val_loss: 276.2318 - val_mse: 276.0881\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 298.9779 - mse: 300.3944 - val_loss: 275.1371 - val_mse: 275.0117\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 300.1142 - mse: 299.2462 - val_loss: 273.9984 - val_mse: 273.8921\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 297.8216 - mse: 298.0620 - val_loss: 272.8867 - val_mse: 272.7990\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 291.8769 - mse: 296.9069 - val_loss: 271.7580 - val_mse: 271.6893\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 296.4272 - mse: 295.7270 - val_loss: 270.5955 - val_mse: 270.5465\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 296.1481 - mse: 294.5504 - val_loss: 269.4938 - val_mse: 269.4635\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 294.1552 - mse: 293.4156 - val_loss: 268.4135 - val_mse: 268.4015\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 292.3631 - mse: 292.2823 - val_loss: 267.3332 - val_mse: 267.3396\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 292.5201 - mse: 291.1637 - val_loss: 266.2447 - val_mse: 266.2697\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 290.5369 - mse: 290.0487 - val_loss: 265.1733 - val_mse: 265.2166\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 289.1691 - mse: 288.9330 - val_loss: 264.0963 - val_mse: 264.1582\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 288.8864 - mse: 287.8136 - val_loss: 263.0156 - val_mse: 263.0961\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 287.6224 - mse: 286.7029 - val_loss: 261.9500 - val_mse: 262.0489\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 287.2301 - mse: 285.5927 - val_loss: 260.8956 - val_mse: 261.0127\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 282.2745 - mse: 284.5110 - val_loss: 259.8533 - val_mse: 259.9885\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 280.7619 - mse: 283.4100 - val_loss: 258.7646 - val_mse: 258.9188\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 282.9627 - mse: 282.2825 - val_loss: 257.6960 - val_mse: 257.8689\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 279.9342 - mse: 281.1830 - val_loss: 256.6366 - val_mse: 256.8281\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 281.2895 - mse: 280.0874 - val_loss: 255.5774 - val_mse: 255.7874\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 275.1388 - mse: 278.9911 - val_loss: 254.5386 - val_mse: 254.7669\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 279.1144 - mse: 277.8875 - val_loss: 253.4608 - val_mse: 253.7081\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 273.9620 - mse: 276.7832 - val_loss: 252.4309 - val_mse: 252.6966\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 274.6238 - mse: 275.7036 - val_loss: 251.3646 - val_mse: 251.6492\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 276.4520 - mse: 274.6140 - val_loss: 250.3047 - val_mse: 250.6082\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 273.3071 - mse: 273.5321 - val_loss: 249.3047 - val_mse: 249.6261\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 274.1143 - mse: 272.4728 - val_loss: 248.2951 - val_mse: 248.6345\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 266.7856 - mse: 271.4415 - val_loss: 247.2950 - val_mse: 247.6525\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 267.4769 - mse: 270.3770 - val_loss: 246.2564 - val_mse: 246.6326\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 269.3874 - mse: 269.2889 - val_loss: 245.2385 - val_mse: 245.6331\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 265.5717 - mse: 268.2556 - val_loss: 244.2264 - val_mse: 244.6393\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 269.0032 - mse: 267.2049 - val_loss: 243.2175 - val_mse: 243.6488\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 267.7697 - mse: 266.1767 - val_loss: 242.2520 - val_mse: 242.7009\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 266.9176 - mse: 265.1798 - val_loss: 241.2995 - val_mse: 241.7658\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 266.3239 - mse: 264.2048 - val_loss: 240.3653 - val_mse: 240.8487\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 260.5805 - mse: 263.2312 - val_loss: 239.4375 - val_mse: 239.9380\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 262.1824 - mse: 262.2332 - val_loss: 238.4466 - val_mse: 238.9655\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 261.4085 - mse: 261.2033 - val_loss: 237.4787 - val_mse: 238.0154\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 259.2868 - mse: 260.1880 - val_loss: 236.5083 - val_mse: 237.0631\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 260.5757 - mse: 259.1907 - val_loss: 235.5136 - val_mse: 236.0869\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 257.9119 - mse: 258.1814 - val_loss: 234.5716 - val_mse: 235.1624\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 258.9265 - mse: 257.1921 - val_loss: 233.6400 - val_mse: 234.2482\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 257.3371 - mse: 256.2260 - val_loss: 232.7204 - val_mse: 233.3460\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 256.7851 - mse: 255.2691 - val_loss: 231.7892 - val_mse: 232.4323\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 255.8562 - mse: 254.3081 - val_loss: 230.8709 - val_mse: 231.5313\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 252.6516 - mse: 253.3495 - val_loss: 229.9665 - val_mse: 230.6440\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 252.2749 - mse: 252.3853 - val_loss: 229.0354 - val_mse: 229.7306\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 253.2684 - mse: 251.4240 - val_loss: 228.0899 - val_mse: 228.8031\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 247.4889 - mse: 250.4571 - val_loss: 227.1855 - val_mse: 227.9160\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 245.4426 - mse: 249.4895 - val_loss: 226.2423 - val_mse: 226.9909\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 249.7688 - mse: 248.5048 - val_loss: 225.2727 - val_mse: 226.0399\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 248.9319 - mse: 247.5129 - val_loss: 224.3628 - val_mse: 225.1475\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 247.2556 - mse: 246.5836 - val_loss: 223.4640 - val_mse: 224.2662\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 244.8832 - mse: 245.6299 - val_loss: 222.5762 - val_mse: 223.3956\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 245.2764 - mse: 244.6930 - val_loss: 221.6619 - val_mse: 222.4991\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 245.0473 - mse: 243.7549 - val_loss: 220.7512 - val_mse: 221.6061\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 244.3665 - mse: 242.8178 - val_loss: 219.8685 - val_mse: 220.7407\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 242.5954 - mse: 241.9124 - val_loss: 218.9968 - val_mse: 219.8861\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 242.0309 - mse: 241.0033 - val_loss: 218.1431 - val_mse: 219.0492\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb35a8bb7b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K6Dfd2-czoe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "faff7c71-f93b-45e1-9375-54d499f95a31"
      },
      "source": [
        "loss, mse = model.evaluate(test_ds)\n",
        "print(\"Mean Squared Error - Test Data\", mse)\n",
        "print(\"loss - Test Data\", loss)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 581.8161 - mse: 575.3245\n",
            "Mean Squared Error - Test Data 575.32446\n",
            "loss - Test Data 581.8161468505859\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu4aIOnRc4WG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#10 question\n",
        "features = ['CRIM','ZN',\t'INDUS',\t'CHAS',\t'NOX',\t'RM',\t'AGE',\t'DIS',\t'RAD',\t'TAX',\t'PTRATIO',\t'B',\t'LSTAT']\n",
        "RAD=tf.feature_column.numeric_column('RAD')\n",
        "feature_columns = []\n",
        "for feature in features:\n",
        "  if feature!='RAD':\n",
        "    feature_columns.append(tf.feature_column.numeric_column(key=feature))\n",
        "  else:\n",
        "    feature_columns.append(tf.feature_column.bucketized_column(RAD, boundaries = [2, 5]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dcD8-S1gsy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
        "# Build and compile your model in this cell.\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "def build_model():\n",
        "  model = keras.Sequential()\n",
        "  model.add(feature_layer),\n",
        "  model.add(layers.Dense(1, activation=None))\n",
        "  \n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "  model.compile(loss='mse',\n",
        "                optimizer=optimizer,\n",
        "                metrics=['mse'])\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoWyYKPVm3NG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBR67Wofm7D1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "71cfca14-934a-4ebd-f1e9-b4a1a0cd62dd"
      },
      "source": [
        "model.fit(train_ds, validation_data=val_ds, epochs=200)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer sequential_10 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Epoch 1/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 34985.5202 - mse: 33743.7773 - val_loss: 0.0000e+00 - val_mse: 0.0000e+00\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 30620.9049 - mse: 30441.4121 - val_loss: 25296.3099 - val_mse: 26674.8398\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 26980.1068 - mse: 27377.2812 - val_loss: 22693.1787 - val_mse: 23996.2168\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 24630.9455 - mse: 24511.5527 - val_loss: 20283.2347 - val_mse: 21511.8789\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 21947.4549 - mse: 21962.9453 - val_loss: 18187.9264 - val_mse: 19348.7969\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 19702.7785 - mse: 19670.6816 - val_loss: 16294.3906 - val_mse: 17389.8633\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 17708.0173 - mse: 17619.2949 - val_loss: 14602.8421 - val_mse: 15635.7314\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 15724.9970 - mse: 15824.8145 - val_loss: 13106.0859 - val_mse: 14079.7871\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 14268.7652 - mse: 14171.3809 - val_loss: 11728.8577 - val_mse: 12644.0430\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 12716.0773 - mse: 12694.7188 - val_loss: 10588.3947 - val_mse: 11451.5703\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 11465.6882 - mse: 11456.2959 - val_loss: 9553.4383 - val_mse: 10365.5928\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 10160.8637 - mse: 10305.7568 - val_loss: 8650.7922 - val_mse: 9414.9629\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9383.5324 - mse: 9327.6035 - val_loss: 7814.7916 - val_mse: 8530.1660\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8301.9133 - mse: 8421.0703 - val_loss: 7149.2856 - val_mse: 7822.6313\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7688.6502 - mse: 7665.8901 - val_loss: 6539.2201 - val_mse: 7170.6533\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6922.2502 - mse: 6999.6724 - val_loss: 6031.9229 - val_mse: 6625.4180\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6409.7505 - mse: 6449.0308 - val_loss: 5597.0868 - val_mse: 6154.9668\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5924.0604 - mse: 5958.9810 - val_loss: 5225.8707 - val_mse: 5750.6699\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5493.3861 - mse: 5552.0928 - val_loss: 4886.7074 - val_mse: 5378.6992\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5181.2579 - mse: 5158.2119 - val_loss: 4609.8873 - val_mse: 5072.5469\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4861.2864 - mse: 4855.2485 - val_loss: 4392.1815 - val_mse: 4829.9131\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4572.6906 - mse: 4594.7231 - val_loss: 4203.2419 - val_mse: 4617.6479\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4401.1227 - mse: 4370.4155 - val_loss: 4034.1461 - val_mse: 4426.2446\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4211.1153 - mse: 4186.9434 - val_loss: 3896.8054 - val_mse: 4269.8628\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4055.7334 - mse: 4024.0061 - val_loss: 3780.9924 - val_mse: 4137.6309\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3895.6294 - mse: 3886.4167 - val_loss: 3673.2923 - val_mse: 4013.8857\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3739.2360 - mse: 3751.3633 - val_loss: 3572.7590 - val_mse: 3897.1038\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3594.5148 - mse: 3621.2019 - val_loss: 3476.8360 - val_mse: 3784.6401\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3506.9199 - mse: 3504.6123 - val_loss: 3387.6179 - val_mse: 3679.3970\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3417.3940 - mse: 3401.4517 - val_loss: 3309.7325 - val_mse: 3588.3877\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3332.0884 - mse: 3308.4072 - val_loss: 3241.3382 - val_mse: 3508.4692\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3238.6705 - mse: 3227.0015 - val_loss: 3178.9856 - val_mse: 3437.0146\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3176.1224 - mse: 3154.9907 - val_loss: 3115.3268 - val_mse: 3364.4094\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3104.4102 - mse: 3080.9629 - val_loss: 3056.4352 - val_mse: 3297.7913\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3022.1744 - mse: 3013.2178 - val_loss: 2999.0029 - val_mse: 3232.7598\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2954.1009 - mse: 2949.4521 - val_loss: 2939.0092 - val_mse: 3164.0896\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2896.1069 - mse: 2879.9121 - val_loss: 2881.0254 - val_mse: 3098.7017\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2835.3010 - mse: 2817.7402 - val_loss: 2824.1797 - val_mse: 3035.6355\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2762.1475 - mse: 2755.5879 - val_loss: 2769.6917 - val_mse: 2975.8123\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2692.3826 - mse: 2696.6208 - val_loss: 2715.2238 - val_mse: 2914.9104\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2603.5710 - mse: 2633.7380 - val_loss: 2657.3552 - val_mse: 2851.4297\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2580.4580 - mse: 2572.7676 - val_loss: 2598.1492 - val_mse: 2783.7148\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2520.8737 - mse: 2510.7957 - val_loss: 2542.6090 - val_mse: 2721.8821\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2469.6603 - mse: 2452.6689 - val_loss: 2487.0439 - val_mse: 2662.8560\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2410.1164 - mse: 2397.4209 - val_loss: 2436.5020 - val_mse: 2608.8423\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2351.1497 - mse: 2346.2620 - val_loss: 2386.6370 - val_mse: 2554.1104\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2304.3833 - mse: 2294.9324 - val_loss: 2337.5175 - val_mse: 2499.5933\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2255.8457 - mse: 2242.8943 - val_loss: 2288.1636 - val_mse: 2445.8647\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2197.3588 - mse: 2192.9519 - val_loss: 2237.6793 - val_mse: 2392.2239\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2114.2593 - mse: 2142.8801 - val_loss: 2186.8608 - val_mse: 2337.3574\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2069.9851 - mse: 2088.7478 - val_loss: 2135.1000 - val_mse: 2279.4919\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2037.3764 - mse: 2033.5846 - val_loss: 2084.7724 - val_mse: 2223.9646\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1960.1725 - mse: 1984.3572 - val_loss: 2037.2172 - val_mse: 2171.7261\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1939.1473 - mse: 1933.9653 - val_loss: 1988.4796 - val_mse: 2118.7976\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1897.8838 - mse: 1888.5701 - val_loss: 1941.6097 - val_mse: 2068.3091\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1851.8328 - mse: 1843.4031 - val_loss: 1896.7462 - val_mse: 2020.2872\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1808.8167 - mse: 1800.2153 - val_loss: 1852.2939 - val_mse: 1973.9977\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1735.8131 - mse: 1758.7314 - val_loss: 1808.3435 - val_mse: 1927.3563\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1725.6979 - mse: 1716.1056 - val_loss: 1762.3529 - val_mse: 1877.7600\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1663.5522 - mse: 1672.4855 - val_loss: 1720.2349 - val_mse: 1832.8391\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1634.0887 - mse: 1631.4808 - val_loss: 1678.8304 - val_mse: 1786.8987\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1595.8624 - mse: 1590.3901 - val_loss: 1641.0475 - val_mse: 1744.8182\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1552.0326 - mse: 1552.9429 - val_loss: 1604.1698 - val_mse: 1703.9153\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1525.1153 - mse: 1516.1418 - val_loss: 1567.6750 - val_mse: 1663.3831\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1487.5273 - mse: 1478.4578 - val_loss: 1529.8418 - val_mse: 1623.5315\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1449.6467 - mse: 1444.0237 - val_loss: 1492.3666 - val_mse: 1584.2861\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1420.4165 - mse: 1410.6829 - val_loss: 1456.2850 - val_mse: 1545.7042\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1384.4068 - mse: 1376.4745 - val_loss: 1421.9257 - val_mse: 1509.7853\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1355.9083 - mse: 1345.8138 - val_loss: 1388.2408 - val_mse: 1473.9000\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1318.7411 - mse: 1314.7018 - val_loss: 1356.1830 - val_mse: 1439.1035\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1258.6684 - mse: 1283.6683 - val_loss: 1322.5647 - val_mse: 1403.3003\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1223.2354 - mse: 1250.2382 - val_loss: 1286.6785 - val_mse: 1364.2959\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1222.5473 - mse: 1215.2133 - val_loss: 1254.8658 - val_mse: 1328.0581\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1193.5110 - mse: 1185.1206 - val_loss: 1222.7438 - val_mse: 1293.8645\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1160.0793 - mse: 1156.1268 - val_loss: 1193.3153 - val_mse: 1262.5056\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1096.0820 - mse: 1128.6985 - val_loss: 1165.2733 - val_mse: 1232.1179\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1107.8592 - mse: 1100.5544 - val_loss: 1135.9026 - val_mse: 1198.9092\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1079.3043 - mse: 1073.6600 - val_loss: 1108.3826 - val_mse: 1169.0601\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1029.7516 - mse: 1047.7190 - val_loss: 1081.5693 - val_mse: 1140.4127\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1027.4979 - mse: 1022.8105 - val_loss: 1050.3769 - val_mse: 1107.2610\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 996.3986 - mse: 994.6843 - val_loss: 1024.0774 - val_mse: 1079.7339\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 980.6899 - mse: 972.9175 - val_loss: 998.8358 - val_mse: 1052.4966\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 951.6995 - mse: 949.3196 - val_loss: 973.9731 - val_mse: 1026.9224\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 908.5699 - mse: 927.7067 - val_loss: 949.1816 - val_mse: 1000.8758\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 899.4725 - mse: 904.7814 - val_loss: 923.0177 - val_mse: 972.4037\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 877.6470 - mse: 881.4650 - val_loss: 900.4503 - val_mse: 947.8208\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 862.8743 - mse: 860.7098 - val_loss: 876.3535 - val_mse: 922.9380\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 831.1050 - mse: 841.4122 - val_loss: 855.1981 - val_mse: 899.7947\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 826.5974 - mse: 820.8980 - val_loss: 833.2466 - val_mse: 876.0092\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 807.8435 - mse: 801.8542 - val_loss: 813.9092 - val_mse: 855.1968\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 788.7580 - mse: 783.6755 - val_loss: 794.5372 - val_mse: 835.0944\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 770.2749 - mse: 767.4294 - val_loss: 775.0768 - val_mse: 814.5660\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 740.0587 - mse: 750.7818 - val_loss: 755.2009 - val_mse: 793.9753\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 735.5545 - mse: 733.6818 - val_loss: 735.8121 - val_mse: 774.0305\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 721.9763 - mse: 718.0400 - val_loss: 718.6189 - val_mse: 755.8024\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 706.3847 - mse: 703.0342 - val_loss: 702.0691 - val_mse: 737.8419\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 687.5460 - mse: 687.5401 - val_loss: 686.0632 - val_mse: 720.6546\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 676.5627 - mse: 673.3288 - val_loss: 669.9849 - val_mse: 703.3627\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 660.5889 - mse: 659.0941 - val_loss: 655.4055 - val_mse: 687.0578\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 649.4326 - mse: 644.8791 - val_loss: 640.8010 - val_mse: 670.9659\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 633.5679 - mse: 632.0398 - val_loss: 627.1391 - val_mse: 656.0386\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 613.4627 - mse: 619.0286 - val_loss: 611.7559 - val_mse: 640.2232\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 607.9873 - mse: 606.3833 - val_loss: 595.2407 - val_mse: 623.7615\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 587.9096 - mse: 593.7347 - val_loss: 581.1398 - val_mse: 608.7135\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 585.4540 - mse: 581.7087 - val_loss: 567.8901 - val_mse: 594.3019\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 573.0921 - mse: 569.9542 - val_loss: 554.9694 - val_mse: 580.8671\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 562.8008 - mse: 559.3928 - val_loss: 542.3320 - val_mse: 567.5243\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 547.7872 - mse: 548.6433 - val_loss: 530.1542 - val_mse: 554.7739\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 542.4494 - mse: 538.1622 - val_loss: 518.2793 - val_mse: 542.2255\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 530.6763 - mse: 528.0800 - val_loss: 507.5078 - val_mse: 530.7136\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 520.4297 - mse: 518.8259 - val_loss: 497.6381 - val_mse: 520.1484\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 511.5975 - mse: 509.6718 - val_loss: 486.1913 - val_mse: 508.5092\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 504.8978 - mse: 501.7987 - val_loss: 474.6277 - val_mse: 497.0749\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 496.1368 - mse: 492.4488 - val_loss: 464.5833 - val_mse: 486.6068\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 486.4262 - mse: 484.6601 - val_loss: 455.4553 - val_mse: 476.5250\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 471.4168 - mse: 475.2683 - val_loss: 446.2362 - val_mse: 466.7723\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 470.0369 - mse: 466.9550 - val_loss: 436.3792 - val_mse: 456.6510\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 460.1810 - mse: 458.9345 - val_loss: 427.3666 - val_mse: 447.1925\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 453.6731 - mse: 451.3037 - val_loss: 418.0226 - val_mse: 437.6650\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 447.2435 - mse: 443.7989 - val_loss: 409.2553 - val_mse: 428.3661\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 433.7775 - mse: 436.3012 - val_loss: 400.8208 - val_mse: 419.5185\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 430.7145 - mse: 429.1582 - val_loss: 392.0538 - val_mse: 410.3211\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 421.7714 - mse: 421.5493 - val_loss: 384.4614 - val_mse: 402.1810\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 409.7964 - mse: 414.9455 - val_loss: 377.3625 - val_mse: 394.5152\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 410.1085 - mse: 408.2135 - val_loss: 369.3644 - val_mse: 386.1020\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 395.7281 - mse: 401.1796 - val_loss: 361.6624 - val_mse: 378.1679\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 391.2331 - mse: 394.3257 - val_loss: 353.6213 - val_mse: 369.8081\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 390.1003 - mse: 387.9930 - val_loss: 345.6919 - val_mse: 361.8564\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 371.0790 - mse: 382.2925 - val_loss: 339.1153 - val_mse: 354.6639\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 376.3098 - mse: 375.8060 - val_loss: 332.5826 - val_mse: 348.4182\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 373.4483 - mse: 371.4521 - val_loss: 326.6220 - val_mse: 342.1639\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 368.1188 - mse: 366.0586 - val_loss: 320.9986 - val_mse: 336.2948\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 361.9238 - mse: 360.9318 - val_loss: 315.5744 - val_mse: 330.3656\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 351.1652 - mse: 355.4444 - val_loss: 309.8782 - val_mse: 324.3648\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 346.1903 - mse: 350.0362 - val_loss: 303.3318 - val_mse: 317.2988\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 340.1393 - mse: 343.3476 - val_loss: 297.5644 - val_mse: 310.8850\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 338.8176 - mse: 338.3506 - val_loss: 291.2982 - val_mse: 304.1864\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 334.1018 - mse: 333.3200 - val_loss: 285.6446 - val_mse: 298.2695\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 330.2229 - mse: 328.0262 - val_loss: 279.3070 - val_mse: 292.0971\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 320.4888 - mse: 323.7649 - val_loss: 274.0487 - val_mse: 286.7905\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 321.1588 - mse: 319.3259 - val_loss: 269.4583 - val_mse: 282.0110\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 316.7102 - mse: 315.2089 - val_loss: 264.6433 - val_mse: 277.2198\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 313.2510 - mse: 311.5967 - val_loss: 260.0395 - val_mse: 272.7111\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 310.7317 - mse: 308.1224 - val_loss: 255.9466 - val_mse: 268.4842\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 304.5021 - mse: 304.4115 - val_loss: 251.9810 - val_mse: 264.4739\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 302.6699 - mse: 300.8548 - val_loss: 248.2422 - val_mse: 260.5413\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 298.7671 - mse: 297.4356 - val_loss: 244.5132 - val_mse: 256.7710\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 296.8969 - mse: 294.4908 - val_loss: 240.3251 - val_mse: 252.7220\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 292.7720 - mse: 291.0450 - val_loss: 236.8114 - val_mse: 249.1478\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 290.2825 - mse: 287.9731 - val_loss: 233.3533 - val_mse: 245.6146\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 286.7059 - mse: 284.7816 - val_loss: 229.9547 - val_mse: 242.2435\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 283.8625 - mse: 282.0830 - val_loss: 226.5698 - val_mse: 238.8163\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 280.7706 - mse: 279.1541 - val_loss: 223.4057 - val_mse: 235.6367\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 272.6705 - mse: 276.1042 - val_loss: 220.4580 - val_mse: 232.5393\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 273.8682 - mse: 273.7811 - val_loss: 217.8589 - val_mse: 229.6584\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 272.4177 - mse: 270.5688 - val_loss: 214.6891 - val_mse: 226.4326\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 269.6189 - mse: 267.7434 - val_loss: 211.4320 - val_mse: 223.1859\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 265.1545 - mse: 264.8716 - val_loss: 208.2205 - val_mse: 220.0859\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 262.3854 - mse: 262.2470 - val_loss: 205.4818 - val_mse: 217.3917\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 259.9087 - mse: 260.4383 - val_loss: 202.6499 - val_mse: 214.8014\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 259.0666 - mse: 257.5021 - val_loss: 200.3025 - val_mse: 212.3996\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 256.4256 - mse: 255.1431 - val_loss: 197.9126 - val_mse: 209.9579\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 253.0274 - mse: 252.9549 - val_loss: 196.0230 - val_mse: 207.9307\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 250.2483 - mse: 250.8867 - val_loss: 193.5912 - val_mse: 205.5111\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 249.4963 - mse: 248.4787 - val_loss: 191.0199 - val_mse: 202.8730\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 247.2014 - mse: 245.9403 - val_loss: 188.1847 - val_mse: 200.1822\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 245.4171 - mse: 243.7799 - val_loss: 185.9494 - val_mse: 197.9377\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 243.3306 - mse: 241.6188 - val_loss: 183.7076 - val_mse: 195.7660\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 238.5303 - mse: 239.6424 - val_loss: 181.4774 - val_mse: 193.5622\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 234.8954 - mse: 237.5437 - val_loss: 178.9420 - val_mse: 191.0054\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 235.3383 - mse: 235.4656 - val_loss: 176.1956 - val_mse: 188.4434\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 233.7818 - mse: 232.9081 - val_loss: 174.3144 - val_mse: 186.4510\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 232.5728 - mse: 231.0228 - val_loss: 172.6859 - val_mse: 184.7286\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 229.5613 - mse: 229.3446 - val_loss: 170.7503 - val_mse: 182.8549\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 227.1127 - mse: 227.5019 - val_loss: 168.6284 - val_mse: 180.8665\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 227.0053 - mse: 225.9386 - val_loss: 166.7699 - val_mse: 179.1501\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 223.9734 - mse: 224.1026 - val_loss: 165.1843 - val_mse: 177.5421\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 223.2385 - mse: 222.4371 - val_loss: 163.6355 - val_mse: 176.0308\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 219.1822 - mse: 220.7065 - val_loss: 162.0308 - val_mse: 174.4756\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 220.7382 - mse: 219.1879 - val_loss: 160.3928 - val_mse: 172.9059\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 218.4047 - mse: 217.5847 - val_loss: 158.8841 - val_mse: 171.3925\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 214.9530 - mse: 216.0091 - val_loss: 157.4045 - val_mse: 169.8970\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 216.1509 - mse: 214.4263 - val_loss: 155.7256 - val_mse: 168.2252\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 214.1062 - mse: 212.7505 - val_loss: 154.3115 - val_mse: 166.7607\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 212.7423 - mse: 211.3112 - val_loss: 153.1643 - val_mse: 165.5732\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 211.7628 - mse: 210.1129 - val_loss: 151.9582 - val_mse: 164.3703\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 209.4653 - mse: 208.7176 - val_loss: 150.6069 - val_mse: 163.0956\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 208.0602 - mse: 207.4092 - val_loss: 149.4221 - val_mse: 161.9222\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 206.9822 - mse: 205.9319 - val_loss: 148.0213 - val_mse: 160.6360\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 204.6780 - mse: 204.5773 - val_loss: 146.8344 - val_mse: 159.5623\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 201.1249 - mse: 203.3673 - val_loss: 145.5931 - val_mse: 158.3263\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 202.0845 - mse: 202.2351 - val_loss: 144.7487 - val_mse: 157.5361\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 200.5267 - mse: 200.9333 - val_loss: 143.9468 - val_mse: 156.8454\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 201.8783 - mse: 200.3494 - val_loss: 143.0880 - val_mse: 156.0764\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 200.2855 - mse: 199.0120 - val_loss: 141.6830 - val_mse: 154.6044\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 198.3521 - mse: 197.2996 - val_loss: 140.4672 - val_mse: 153.2964\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 196.9277 - mse: 196.0269 - val_loss: 139.3307 - val_mse: 152.0820\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 195.9143 - mse: 194.7711 - val_loss: 138.2120 - val_mse: 150.9773\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 190.3139 - mse: 193.5682 - val_loss: 137.2722 - val_mse: 150.0515\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 193.3564 - mse: 192.6786 - val_loss: 136.3990 - val_mse: 149.0829\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb358dd4710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3lrMbz_m-NA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "8dbdc608-c1b3-4192-eac4-0758c60a6f09"
      },
      "source": [
        "oss, mse = model.evaluate(test_ds)\n",
        "print(\"Mean Squared Error - Test Data\", mse)\n",
        "print(\"loss - Test Data\", loss)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 152.2631 - mse: 142.5060\n",
            "Mean Squared Error - Test Data 142.50598\n",
            "loss - Test Data 581.8161468505859\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMgkvRgInD6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}